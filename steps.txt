
############################  process train data ################################

- def add_beginAndEnd_to_each_sentense
  takes in self.corpus
   add '</s>', '<s>' to the end of each sentence,
   add '<s>'' to the beginning of each of 50 line, add '</s>' to the end of each of 50 line
  gives out self.beginEnd


- def flatten_text
  takes in self.beginEnd
   change the list list of 50, to list of 1, with all elements in it
  gives out self.beginEndFlat


- def least_freq_to_UNK
  takes in self.beginEndFlat
   replace words which total appearence less than min_freq times by 'UNK' (think, replace UNK for total appearence, or for appearence in each sentense? This can be adjusted by change original input text from self.beginEndFlat to self.eachSentense)
  gives out self.UNKreplaced


- def divide_each_sentense
  takes in self.UNKreplaceBeginEnd
   self.UNKreplacedBeginEnd is adding a </s> to the very front, and removed a </s> at very end
  gives out self.eachSentense, which is a list list of sentenses

##################################  build  ###################################

- def build
  takes in self.eachSentense
    build *list of tuples* for uniform, unigram, bigram, trigram, from input text
    uniform: single words, exclude </s> <s>
    unigram: single words, exclude </s> <s>
    bigram: two words in each tuple, with single words for i-1
    trigram: three words in each tuple, with two-words-tuple for i-1, i-2
  we get self.ngramList

- def build
  use package "from collections import Counter", 
  use the Counter function to make lists above into dictionary form
  we get self.ngramDict

############################  process test data ################################

- process test data
  in test data, any word that is not in vocabulary is replaced with 'UNK'
  add </s> <s> to sentence of test data

##########################  calculate perplexity ################################

- def calculate_perplexity
  calculate perplexity for uniorm, unigram, bigram, trigram,
  according to dictionary counts

- Take care of zero appearence problem using laplace smoothing (+1 to the top, +V to the bottom)

- Math:
  a = get probability for each individual word for each ngram
  b = multiply a with coef
  c = add b (we have four of them, one for each ngram)
  d = take log of c
  d is for one token (word), do d for every word in TEST dataset
  e = add d (number of d is the number of tokens in the TEST data)
  f = e * -1
  g = f / number of tokens in the test data
  h = 2**g













